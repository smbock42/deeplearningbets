{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys8EJg7PLYyC"
      },
      "source": [
        "I chose to make a model that takes in 5 years of stock data and will decide whether to buy, sell, or hold that stock position.\n",
        "\n",
        "I am comparing a baseline Linear NN with two more advanced models to see whether predictions and accuracy improve with more complexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCSqD4IkoN2c",
        "outputId": "179dd6ae-77a5-4be1-f84a-c44af74da130"
      },
      "outputs": [],
      "source": [
        "%pip install pandas numpy yfinance scikit-learn torch matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7dIjfD-oZ4d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD6fo7RWMi6B"
      },
      "source": [
        "# Choose Stock and Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ4-SrhJoe7p",
        "outputId": "d95391c1-4edc-4199-cf59-850d0eeca33f"
      },
      "outputs": [],
      "source": [
        "stock_ticker = 'TSLA'  # Stock Ticker to predict buy, sell, or hold\n",
        "sp500_ticker = '^GSPC'  # S&P 500 index\n",
        "\n",
        "start_date = '2015-01-01'\n",
        "end_date = datetime.date.today().strftime('%Y-%m-%d')\n",
        "# Fetch the data using yfinance\n",
        "stock_data = yf.download(stock_ticker, start=start_date, end=end_date)\n",
        "sp500_data = yf.download(sp500_ticker, start=start_date, end=end_date)\n",
        "\n",
        "data = stock_data[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
        "data['SP500_Close'] = sp500_data['Close']\n",
        "\n",
        "data.columns = data.columns.get_level_values(0)\n",
        "\n",
        "data.ffill(inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr1vXYpsMmWl"
      },
      "source": [
        "# Create Buy, Sell, and Hold Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFdMeYJysYdC"
      },
      "outputs": [],
      "source": [
        "THRESHOLD = 0.003 #hold threshold if open and close are within .5% of each other\n",
        "\n",
        "\n",
        "def label_data(row):\n",
        "    change = abs((row['Close'] - row['Open']) / row['Open'])\n",
        "    if change < THRESHOLD:\n",
        "        return 0\n",
        "    elif row['Open'] < row['Close']:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "data['Label']  = data.apply(label_data, axis=1)\n",
        "data['Next_Open'] = data['Open'].shift(-1)\n",
        "data['Next_Label'] = data['Label'].shift(-1)\n",
        "\n",
        "data.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5ucuPYbMqGi"
      },
      "source": [
        "# Setup Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0k8mZUqt137"
      },
      "outputs": [],
      "source": [
        "features = ['Open', 'High', 'Low', 'Close', 'Volume', 'SP500_Close']\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(data[features])\n",
        "y = data['Label'].values\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "batch_size = 1024\n",
        "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP_RJ3sQMfg7"
      },
      "source": [
        "# Baseline Linear Neural Network Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40obro0_Ex9B"
      },
      "outputs": [],
      "source": [
        "class BaseLineNN(nn.Module):\n",
        "    def __init__(self,  input_dim, output_dim):\n",
        "        super(BaseLineNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 16)\n",
        "        self.fc2 = nn.Linear(16, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "input_dim = len(features)\n",
        "output_dim = 3\n",
        "baseline_model = BaseLineNN(input_dim, output_dim)\n",
        "\n",
        "baseline_criterion = nn.CrossEntropyLoss()\n",
        "baseline_optimizer = optim.SGD(baseline_model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLbbYEhmH3Lv",
        "outputId": "354c3dfa-da79-4d70-c9a6-8c7695e8171d"
      },
      "outputs": [],
      "source": [
        "num_epochs = 1000\n",
        "best_baseline_loss = float(\"inf\")\n",
        "baseline_model_path = \"best_baseline_model.pth\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    baseline_model.train()\n",
        "    total_loss = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        baseline_optimizer.zero_grad()\n",
        "        outputs = baseline_model(inputs)\n",
        "        loss = baseline_criterion(outputs, labels + 1)\n",
        "        loss.backward()\n",
        "        baseline_optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_train += (predicted == (labels + 1)).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    train_accuracy = correct_train / total_train\n",
        "\n",
        "    baseline_model.eval()\n",
        "    total_val_loss = 0\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = baseline_model(inputs)\n",
        "            val_loss = baseline_criterion(outputs, labels + 1)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_test += (predicted == (labels + 1)).sum().item()\n",
        "            total_test += labels.size(0)\n",
        "\n",
        "    val_accuracy = correct_test / total_test\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    avg_val_loss = total_val_loss / len(test_loader)\n",
        "\n",
        "    if avg_val_loss < best_baseline_loss:\n",
        "        best_baseline_loss = avg_val_loss\n",
        "        torch.save(baseline_model.state_dict(), baseline_model_path)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Baseline Epoch [{epoch+1}/{num_epochs}] | \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4%} | \"\n",
        "              f\"Test Loss: {avg_val_loss:.4f} | Test Acc: {val_accuracy:.4%}\")\n",
        "\n",
        "print(f\"\\nBest baseline model saved to: {baseline_model_path} with Test Loss: {best_baseline_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZXS9zB4OKib"
      },
      "source": [
        "# Medium Neural Network Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpvCb8TBOQ7i"
      },
      "outputs": [],
      "source": [
        "class MediumClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(MediumClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)  # No activation (CrossEntropyLoss expects raw logits)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "input_dim = len(features)\n",
        "output_dim = 3  # Buy, Sell, Hold\n",
        "mediumClassifier = MediumClassifier(input_dim, output_dim)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
        "optimizer = optim.Adam(mediumClassifier.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bIJ7j72OVMc",
        "outputId": "973d7407-f6dd-4b69-a1ef-ebc3a3d6a6cf"
      },
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "num_epochs = 1000\n",
        "best_val_loss = float(\"inf\")  # Track lowest validation loss\n",
        "best_model_path = \"best_stock_model.pth\"  # Save model path\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    mediumClassifier.train()\n",
        "    total_loss = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    # Training loop\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mediumClassifier(inputs)\n",
        "        loss = criterion(outputs, labels + 1)  # Shift labels (-1,0,1) â†’ (0,1,2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Compute training accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_train += (predicted == (labels + 1)).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    train_accuracy = correct_train / total_train\n",
        "\n",
        "    # Validation loop\n",
        "    mediumClassifier.eval()\n",
        "    total_val_loss = 0\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = mediumClassifier(inputs)\n",
        "            val_loss = criterion(outputs, labels + 1)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            # Compute test accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_test += (predicted == (labels + 1)).sum().item()\n",
        "            total_test += labels.size(0)\n",
        "\n",
        "    val_accuracy = correct_test / total_test\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    avg_val_loss = total_val_loss / len(test_loader)\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(mediumClassifier.state_dict(), best_model_path)\n",
        "\n",
        "    # Print epoch summary\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
        "            f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4%} | \"\n",
        "            f\"Test Loss: {avg_val_loss:.4f} | Test Acc: {val_accuracy:.4%}\")\n",
        "\n",
        "print(f\"\\nBest model saved to: {best_model_path} with Test Loss: {best_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLrQhqV2MuBZ"
      },
      "source": [
        "# Advanced Neural Network Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJZL6mZnueV6"
      },
      "outputs": [],
      "source": [
        "class ImprovedStockClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ImprovedStockClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.bn3 = nn.BatchNorm1d(32)\n",
        "        self.dropout3 = nn.Dropout(0.2)\n",
        "\n",
        "        self.fc4 = nn.Linear(32, output_dim)\n",
        "\n",
        "        self.activation = nn.LeakyReLU(negative_slope=0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.activation(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.activation(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "improved_model = ImprovedStockClassifier(input_dim, output_dim)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(improved_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqqsbashHNWL",
        "outputId": "5950301c-987e-41cb-b3ca-16db1e043687"
      },
      "outputs": [],
      "source": [
        "num_epochs = 1000\n",
        "best_improved_loss = float(\"inf\")\n",
        "improved_model_path = \"best_improved_model.pth\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    improved_model.train()\n",
        "    total_loss = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = improved_model(inputs)\n",
        "        loss = criterion(outputs, labels + 1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_train += (predicted == (labels + 1)).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    train_accuracy = correct_train / total_train\n",
        "\n",
        "    improved_model.eval()\n",
        "    total_val_loss = 0\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = improved_model(inputs)\n",
        "            val_loss = criterion(outputs, labels + 1)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_test += (predicted == (labels + 1)).sum().item()\n",
        "            total_test += labels.size(0)\n",
        "\n",
        "    val_accuracy = correct_test / total_test\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    avg_val_loss = total_val_loss / len(test_loader)\n",
        "\n",
        "    if avg_val_loss < best_improved_loss:\n",
        "        best_improved_loss = avg_val_loss\n",
        "        torch.save(improved_model.state_dict(), improved_model_path)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Improved Model - Epoch [{epoch+1}/{num_epochs}] | \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4%} | \"\n",
        "              f\"Test Loss: {avg_val_loss:.4f} | Test Acc: {val_accuracy:.4%}\")\n",
        "\n",
        "print(f\"\\nBest improved model saved to: {improved_model_path} with Test Loss: {best_improved_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir0KMvfLNL6F"
      },
      "source": [
        "# Compare and Evalutate both models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyZPhQn0ulan",
        "outputId": "6b03397f-2f68-47b0-ade7-f6dd182b7c23"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, name=\"Model\"):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predictions.extend(predicted.numpy())\n",
        "            actuals.extend((labels + 1).numpy())  # Shift back from (-1,0,1) to (0,1,2)\n",
        "\n",
        "    print(f\"{name} Classification Report:\")\n",
        "    print(classification_report(actuals, predictions, target_names=['Sell', 'Hold', 'Buy']))\n",
        "    return predictions, actuals\n",
        "\n",
        "# Evaluate both models\n",
        "predictions_baseline, actuals_baseline = evaluate_model(baseline_model, test_loader, name=\"Baseline NN\")\n",
        "predictions_baseline, actuals_baseline = evaluate_model(mediumClassifier, test_loader, name=\"Baseline NN\")\n",
        "predictions_advanced, actuals_advanced = evaluate_model(improved_model, test_loader, name=\"Advanced NN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY7Pjlb5uxbz",
        "outputId": "bffef682-a24f-4e75-a3ff-3cee7ea45ff2"
      },
      "outputs": [],
      "source": [
        "latest_data = stock_data.iloc[-1][['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "latest_sp500 = sp500_data.iloc[-1]['Close']\n",
        "\n",
        "latest_features = np.array([latest_data['Open'], latest_data['High'], latest_data['Low'], latest_data['Close'], latest_data['Volume'], latest_sp500])\n",
        "latest_features = latest_features.reshape(1, -1)\n",
        "latest_features_scaled = scaler.transform(latest_features)\n",
        "latest_tensor = torch.tensor(latest_features_scaled, dtype=torch.float32)\n",
        "\n",
        "baseline_model.eval()\n",
        "with torch.no_grad():\n",
        "    output = baseline_model(latest_tensor)\n",
        "    _, predicted_class = torch.max(output, 1)\n",
        "\n",
        "label_map = {0: \"Sell\", 1: \"Hold\", 2: \"Buy\"}\n",
        "predicted_label = label_map[predicted_class.item()]\n",
        "\n",
        "print(f\"Baseline model: Today's recommended action for {stock_ticker}: {predicted_label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyS8-_-ROuJc",
        "outputId": "9423a761-5ff2-4f1d-fe30-539045955501"
      },
      "outputs": [],
      "source": [
        "latest_data = stock_data.iloc[-1][['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "latest_sp500 = sp500_data.iloc[-1]['Close']\n",
        "\n",
        "latest_features = np.array([latest_data['Open'], latest_data['High'], latest_data['Low'], latest_data['Close'], latest_data['Volume'], latest_sp500])\n",
        "latest_features = latest_features.reshape(1, -1)\n",
        "latest_features_scaled = scaler.transform(latest_features)\n",
        "latest_tensor = torch.tensor(latest_features_scaled, dtype=torch.float32)\n",
        "\n",
        "mediumClassifier.eval()\n",
        "with torch.no_grad():\n",
        "    output = mediumClassifier(latest_tensor)\n",
        "    _, predicted_class = torch.max(output, 1)\n",
        "\n",
        "label_map = {0: \"Sell\", 1: \"Hold\", 2: \"Buy\"}\n",
        "predicted_label = label_map[predicted_class.item()]\n",
        "\n",
        "print(f\"Medium model: Today's recommended action for {stock_ticker}: {predicted_label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1AKWc3gKo5d",
        "outputId": "5a4065ad-cc95-468a-df6d-25ab7cb2eb1f"
      },
      "outputs": [],
      "source": [
        "latest_data = stock_data.iloc[-1][['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "latest_sp500 = sp500_data.iloc[-1]['Close']\n",
        "\n",
        "latest_features = np.array([latest_data['Open'], latest_data['High'], latest_data['Low'], latest_data['Close'], latest_data['Volume'], latest_sp500])\n",
        "latest_features = latest_features.reshape(1, -1)\n",
        "latest_features_scaled = scaler.transform(latest_features)\n",
        "latest_tensor = torch.tensor(latest_features_scaled, dtype=torch.float32)\n",
        "\n",
        "improved_model.eval()\n",
        "with torch.no_grad():\n",
        "    output = improved_model(latest_tensor)\n",
        "    _, predicted_class = torch.max(output, 1)\n",
        "\n",
        "label_map = {0: \"Sell\", 1: \"Hold\", 2: \"Buy\"}\n",
        "predicted_label = label_map[predicted_class.item()]\n",
        "\n",
        "print(f\"Improved model: Today's recommended action for {stock_ticker}: {predicted_label}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
