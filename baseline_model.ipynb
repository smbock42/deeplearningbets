{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "I chose to make a model that takes in 5 years of stock data and will decide whether to buy, sell, or hold that stock position.\n",
        "\n",
        "I am comparing a baseline Linear NN with two more advanced models to see whether predictions and accuracy improve with more complexity."
      ],
      "metadata": {
        "id": "ys8EJg7PLYyC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCSqD4IkoN2c",
        "outputId": "179dd6ae-77a5-4be1-f84a-c44af74da130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.52)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.6)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas numpy yfinance scikit-learn torch matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M7dIjfD-oZ4d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choose Stock and Download Data"
      ],
      "metadata": {
        "id": "AD6fo7RWMi6B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ4-SrhJoe7p",
        "outputId": "d95391c1-4edc-4199-cf59-850d0eeca33f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "stock_ticker = 'TSLA'  # Stock Ticker to predict buy, sell, or hold\n",
        "sp500_ticker = '^GSPC'  # S&P 500 index\n",
        "\n",
        "start_date = '2015-01-01'\n",
        "end_date = datetime.date.today().strftime('%Y-%m-%d')\n",
        "# Fetch the data using yfinance\n",
        "stock_data = yf.download(stock_ticker, start=start_date, end=end_date)\n",
        "sp500_data = yf.download(sp500_ticker, start=start_date, end=end_date)\n",
        "\n",
        "data = stock_data[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
        "data['SP500_Close'] = sp500_data['Close']\n",
        "\n",
        "data.columns = data.columns.get_level_values(0)\n",
        "\n",
        "data.ffill(inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Buy, Sell, and Hold Labels"
      ],
      "metadata": {
        "id": "qr1vXYpsMmWl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eFdMeYJysYdC"
      },
      "outputs": [],
      "source": [
        "THRESHOLD = 0.003 #hold threshold if open and close are within .5% of each other\n",
        "\n",
        "\n",
        "def label_data(row):\n",
        "    change = abs((row['Close'] - row['Open']) / row['Open'])\n",
        "    if change < THRESHOLD:\n",
        "        return 0\n",
        "    elif row['Open'] < row['Close']:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "data['Label']  = data.apply(label_data, axis=1)\n",
        "data['Next_Open'] = data['Open'].shift(-1)\n",
        "data['Next_Label'] = data['Label'].shift(-1)\n",
        "\n",
        "data.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Dataloaders"
      ],
      "metadata": {
        "id": "T5ucuPYbMqGi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "W0k8mZUqt137"
      },
      "outputs": [],
      "source": [
        "features = ['Open', 'High', 'Low', 'Close', 'Volume', 'SP500_Close']\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(data[features])\n",
        "y = data['Label'].values\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "batch_size = 1024\n",
        "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Linear Neural Network Training"
      ],
      "metadata": {
        "id": "LP_RJ3sQMfg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseLineNN(nn.Module):\n",
        "    def __init__(self,  input_dim, output_dim):\n",
        "        super(BaseLineNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 16)\n",
        "        self.fc2 = nn.Linear(16, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "input_dim = len(features)\n",
        "output_dim = 3\n",
        "baseline_model = BaseLineNN(input_dim, output_dim)\n",
        "\n",
        "baseline_criterion = nn.CrossEntropyLoss()\n",
        "baseline_optimizer = optim.SGD(baseline_model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "40obro0_Ex9B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1000\n",
        "best_baseline_loss = float(\"inf\")\n",
        "baseline_model_path = \"best_baseline_model.pth\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    baseline_model.train()\n",
        "    total_loss = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        baseline_optimizer.zero_grad()\n",
        "        outputs = baseline_model(inputs)\n",
        "        loss = baseline_criterion(outputs, labels + 1)\n",
        "        loss.backward()\n",
        "        baseline_optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_train += (predicted == (labels + 1)).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    train_accuracy = correct_train / total_train\n",
        "\n",
        "    baseline_model.eval()\n",
        "    total_val_loss = 0\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = baseline_model(inputs)\n",
        "            val_loss = baseline_criterion(outputs, labels + 1)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_test += (predicted == (labels + 1)).sum().item()\n",
        "            total_test += labels.size(0)\n",
        "\n",
        "    val_accuracy = correct_test / total_test\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    avg_val_loss = total_val_loss / len(test_loader)\n",
        "\n",
        "    if avg_val_loss < best_baseline_loss:\n",
        "        best_baseline_loss = avg_val_loss\n",
        "        torch.save(baseline_model.state_dict(), baseline_model_path)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Baseline Epoch [{epoch+1}/{num_epochs}] | \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4%} | \"\n",
        "              f\"Test Loss: {avg_val_loss:.4f} | Test Acc: {val_accuracy:.4%}\")\n",
        "\n",
        "print(f\"\\nBest baseline model saved to: {baseline_model_path} with Test Loss: {best_baseline_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLbbYEhmH3Lv",
        "outputId": "354c3dfa-da79-4d70-c9a6-8c7695e8171d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Epoch [1/1000] | Train Loss: 1.1806 | Train Acc: 28.0079% | Test Loss: 1.1738 | Test Acc: 28.7968%\n",
            "Baseline Epoch [11/1000] | Train Loss: 1.1140 | Train Acc: 30.1282% | Test Loss: 1.1099 | Test Acc: 31.3609%\n",
            "Baseline Epoch [21/1000] | Train Loss: 1.0724 | Train Acc: 45.1677% | Test Loss: 1.0696 | Test Acc: 45.1677%\n",
            "Baseline Epoch [31/1000] | Train Loss: 1.0447 | Train Acc: 45.1183% | Test Loss: 1.0427 | Test Acc: 45.1677%\n",
            "Baseline Epoch [41/1000] | Train Loss: 1.0253 | Train Acc: 45.1183% | Test Loss: 1.0238 | Test Acc: 44.9704%\n",
            "Baseline Epoch [51/1000] | Train Loss: 1.0113 | Train Acc: 45.1677% | Test Loss: 1.0100 | Test Acc: 44.7732%\n",
            "Baseline Epoch [61/1000] | Train Loss: 1.0009 | Train Acc: 45.1677% | Test Loss: 0.9997 | Test Acc: 44.5759%\n",
            "Baseline Epoch [71/1000] | Train Loss: 0.9929 | Train Acc: 45.1183% | Test Loss: 0.9918 | Test Acc: 44.5759%\n",
            "Baseline Epoch [81/1000] | Train Loss: 0.9868 | Train Acc: 45.2170% | Test Loss: 0.9856 | Test Acc: 44.9704%\n",
            "Baseline Epoch [91/1000] | Train Loss: 0.9818 | Train Acc: 45.2170% | Test Loss: 0.9806 | Test Acc: 44.9704%\n",
            "Baseline Epoch [101/1000] | Train Loss: 0.9779 | Train Acc: 45.3156% | Test Loss: 0.9766 | Test Acc: 44.9704%\n",
            "Baseline Epoch [111/1000] | Train Loss: 0.9747 | Train Acc: 44.9704% | Test Loss: 0.9733 | Test Acc: 45.1677%\n",
            "Baseline Epoch [121/1000] | Train Loss: 0.9722 | Train Acc: 45.0690% | Test Loss: 0.9706 | Test Acc: 44.9704%\n",
            "Baseline Epoch [131/1000] | Train Loss: 0.9699 | Train Acc: 45.0690% | Test Loss: 0.9683 | Test Acc: 44.9704%\n",
            "Baseline Epoch [141/1000] | Train Loss: 0.9682 | Train Acc: 45.1183% | Test Loss: 0.9664 | Test Acc: 44.9704%\n",
            "Baseline Epoch [151/1000] | Train Loss: 0.9667 | Train Acc: 45.2663% | Test Loss: 0.9648 | Test Acc: 44.7732%\n",
            "Baseline Epoch [161/1000] | Train Loss: 0.9654 | Train Acc: 45.3649% | Test Loss: 0.9633 | Test Acc: 44.9704%\n",
            "Baseline Epoch [171/1000] | Train Loss: 0.9641 | Train Acc: 45.4635% | Test Loss: 0.9621 | Test Acc: 44.9704%\n",
            "Baseline Epoch [181/1000] | Train Loss: 0.9632 | Train Acc: 45.5128% | Test Loss: 0.9610 | Test Acc: 44.7732%\n",
            "Baseline Epoch [191/1000] | Train Loss: 0.9623 | Train Acc: 45.5621% | Test Loss: 0.9601 | Test Acc: 44.7732%\n",
            "Baseline Epoch [201/1000] | Train Loss: 0.9616 | Train Acc: 45.5621% | Test Loss: 0.9592 | Test Acc: 44.9704%\n",
            "Baseline Epoch [211/1000] | Train Loss: 0.9610 | Train Acc: 45.5128% | Test Loss: 0.9585 | Test Acc: 44.7732%\n",
            "Baseline Epoch [221/1000] | Train Loss: 0.9603 | Train Acc: 45.9566% | Test Loss: 0.9578 | Test Acc: 44.7732%\n",
            "Baseline Epoch [231/1000] | Train Loss: 0.9598 | Train Acc: 46.0552% | Test Loss: 0.9572 | Test Acc: 44.7732%\n",
            "Baseline Epoch [241/1000] | Train Loss: 0.9592 | Train Acc: 46.4004% | Test Loss: 0.9566 | Test Acc: 44.3787%\n",
            "Baseline Epoch [251/1000] | Train Loss: 0.9588 | Train Acc: 46.1538% | Test Loss: 0.9561 | Test Acc: 44.5759%\n",
            "Baseline Epoch [261/1000] | Train Loss: 0.9585 | Train Acc: 46.3511% | Test Loss: 0.9557 | Test Acc: 44.5759%\n",
            "Baseline Epoch [271/1000] | Train Loss: 0.9581 | Train Acc: 46.3511% | Test Loss: 0.9552 | Test Acc: 44.1815%\n",
            "Baseline Epoch [281/1000] | Train Loss: 0.9578 | Train Acc: 46.2525% | Test Loss: 0.9548 | Test Acc: 43.9842%\n",
            "Baseline Epoch [291/1000] | Train Loss: 0.9574 | Train Acc: 46.3018% | Test Loss: 0.9544 | Test Acc: 43.5897%\n",
            "Baseline Epoch [301/1000] | Train Loss: 0.9573 | Train Acc: 46.2032% | Test Loss: 0.9541 | Test Acc: 44.1815%\n",
            "Baseline Epoch [311/1000] | Train Loss: 0.9568 | Train Acc: 46.4004% | Test Loss: 0.9538 | Test Acc: 44.5759%\n",
            "Baseline Epoch [321/1000] | Train Loss: 0.9567 | Train Acc: 46.6963% | Test Loss: 0.9535 | Test Acc: 44.5759%\n",
            "Baseline Epoch [331/1000] | Train Loss: 0.9565 | Train Acc: 46.8442% | Test Loss: 0.9532 | Test Acc: 44.5759%\n",
            "Baseline Epoch [341/1000] | Train Loss: 0.9563 | Train Acc: 46.5976% | Test Loss: 0.9529 | Test Acc: 44.9704%\n",
            "Baseline Epoch [351/1000] | Train Loss: 0.9559 | Train Acc: 46.4004% | Test Loss: 0.9527 | Test Acc: 45.1677%\n",
            "Baseline Epoch [361/1000] | Train Loss: 0.9557 | Train Acc: 46.4004% | Test Loss: 0.9524 | Test Acc: 45.3649%\n",
            "Baseline Epoch [371/1000] | Train Loss: 0.9553 | Train Acc: 46.5976% | Test Loss: 0.9522 | Test Acc: 45.7594%\n",
            "Baseline Epoch [381/1000] | Train Loss: 0.9552 | Train Acc: 46.5976% | Test Loss: 0.9520 | Test Acc: 45.9566%\n",
            "Baseline Epoch [391/1000] | Train Loss: 0.9550 | Train Acc: 46.5976% | Test Loss: 0.9517 | Test Acc: 45.9566%\n",
            "Baseline Epoch [401/1000] | Train Loss: 0.9549 | Train Acc: 46.8935% | Test Loss: 0.9515 | Test Acc: 45.9566%\n",
            "Baseline Epoch [411/1000] | Train Loss: 0.9546 | Train Acc: 46.9921% | Test Loss: 0.9513 | Test Acc: 46.3511%\n",
            "Baseline Epoch [421/1000] | Train Loss: 0.9544 | Train Acc: 46.8442% | Test Loss: 0.9511 | Test Acc: 46.3511%\n",
            "Baseline Epoch [431/1000] | Train Loss: 0.9544 | Train Acc: 46.9428% | Test Loss: 0.9510 | Test Acc: 46.5483%\n",
            "Baseline Epoch [441/1000] | Train Loss: 0.9542 | Train Acc: 46.9921% | Test Loss: 0.9508 | Test Acc: 46.7456%\n",
            "Baseline Epoch [451/1000] | Train Loss: 0.9540 | Train Acc: 47.1893% | Test Loss: 0.9506 | Test Acc: 47.1400%\n",
            "Baseline Epoch [461/1000] | Train Loss: 0.9538 | Train Acc: 47.1893% | Test Loss: 0.9504 | Test Acc: 47.1400%\n",
            "Baseline Epoch [471/1000] | Train Loss: 0.9537 | Train Acc: 47.2880% | Test Loss: 0.9503 | Test Acc: 46.7456%\n",
            "Baseline Epoch [481/1000] | Train Loss: 0.9536 | Train Acc: 47.4359% | Test Loss: 0.9501 | Test Acc: 46.5483%\n",
            "Baseline Epoch [491/1000] | Train Loss: 0.9534 | Train Acc: 47.5345% | Test Loss: 0.9500 | Test Acc: 46.7456%\n",
            "Baseline Epoch [501/1000] | Train Loss: 0.9532 | Train Acc: 47.5345% | Test Loss: 0.9498 | Test Acc: 46.7456%\n",
            "Baseline Epoch [511/1000] | Train Loss: 0.9530 | Train Acc: 47.5838% | Test Loss: 0.9497 | Test Acc: 46.7456%\n",
            "Baseline Epoch [521/1000] | Train Loss: 0.9528 | Train Acc: 47.4359% | Test Loss: 0.9496 | Test Acc: 46.9428%\n",
            "Baseline Epoch [531/1000] | Train Loss: 0.9529 | Train Acc: 47.3373% | Test Loss: 0.9494 | Test Acc: 46.9428%\n",
            "Baseline Epoch [541/1000] | Train Loss: 0.9528 | Train Acc: 47.4852% | Test Loss: 0.9493 | Test Acc: 46.9428%\n",
            "Baseline Epoch [551/1000] | Train Loss: 0.9525 | Train Acc: 47.6824% | Test Loss: 0.9492 | Test Acc: 46.9428%\n",
            "Baseline Epoch [561/1000] | Train Loss: 0.9522 | Train Acc: 47.7811% | Test Loss: 0.9490 | Test Acc: 46.9428%\n",
            "Baseline Epoch [571/1000] | Train Loss: 0.9522 | Train Acc: 47.8304% | Test Loss: 0.9489 | Test Acc: 47.1400%\n",
            "Baseline Epoch [581/1000] | Train Loss: 0.9520 | Train Acc: 47.6331% | Test Loss: 0.9488 | Test Acc: 46.9428%\n",
            "Baseline Epoch [591/1000] | Train Loss: 0.9519 | Train Acc: 47.4852% | Test Loss: 0.9487 | Test Acc: 46.7456%\n",
            "Baseline Epoch [601/1000] | Train Loss: 0.9518 | Train Acc: 47.7811% | Test Loss: 0.9486 | Test Acc: 46.5483%\n",
            "Baseline Epoch [611/1000] | Train Loss: 0.9519 | Train Acc: 47.6824% | Test Loss: 0.9485 | Test Acc: 46.9428%\n",
            "Baseline Epoch [621/1000] | Train Loss: 0.9517 | Train Acc: 47.6824% | Test Loss: 0.9484 | Test Acc: 46.9428%\n",
            "Baseline Epoch [631/1000] | Train Loss: 0.9515 | Train Acc: 47.7811% | Test Loss: 0.9483 | Test Acc: 46.9428%\n",
            "Baseline Epoch [641/1000] | Train Loss: 0.9514 | Train Acc: 47.7811% | Test Loss: 0.9482 | Test Acc: 47.1400%\n",
            "Baseline Epoch [651/1000] | Train Loss: 0.9512 | Train Acc: 47.8797% | Test Loss: 0.9481 | Test Acc: 47.3373%\n",
            "Baseline Epoch [661/1000] | Train Loss: 0.9512 | Train Acc: 47.8304% | Test Loss: 0.9480 | Test Acc: 47.3373%\n",
            "Baseline Epoch [671/1000] | Train Loss: 0.9510 | Train Acc: 47.8304% | Test Loss: 0.9479 | Test Acc: 47.3373%\n",
            "Baseline Epoch [681/1000] | Train Loss: 0.9510 | Train Acc: 47.6824% | Test Loss: 0.9478 | Test Acc: 47.5345%\n",
            "Baseline Epoch [691/1000] | Train Loss: 0.9508 | Train Acc: 47.7811% | Test Loss: 0.9477 | Test Acc: 47.3373%\n",
            "Baseline Epoch [701/1000] | Train Loss: 0.9505 | Train Acc: 47.8304% | Test Loss: 0.9476 | Test Acc: 47.1400%\n",
            "Baseline Epoch [711/1000] | Train Loss: 0.9505 | Train Acc: 48.0276% | Test Loss: 0.9475 | Test Acc: 47.3373%\n",
            "Baseline Epoch [721/1000] | Train Loss: 0.9504 | Train Acc: 48.0276% | Test Loss: 0.9474 | Test Acc: 47.3373%\n",
            "Baseline Epoch [731/1000] | Train Loss: 0.9503 | Train Acc: 48.0769% | Test Loss: 0.9473 | Test Acc: 47.3373%\n",
            "Baseline Epoch [741/1000] | Train Loss: 0.9501 | Train Acc: 48.0769% | Test Loss: 0.9473 | Test Acc: 47.5345%\n",
            "Baseline Epoch [751/1000] | Train Loss: 0.9499 | Train Acc: 48.1755% | Test Loss: 0.9472 | Test Acc: 47.5345%\n",
            "Baseline Epoch [761/1000] | Train Loss: 0.9499 | Train Acc: 48.1755% | Test Loss: 0.9471 | Test Acc: 47.5345%\n",
            "Baseline Epoch [771/1000] | Train Loss: 0.9498 | Train Acc: 48.3235% | Test Loss: 0.9470 | Test Acc: 47.5345%\n",
            "Baseline Epoch [781/1000] | Train Loss: 0.9498 | Train Acc: 48.3235% | Test Loss: 0.9469 | Test Acc: 47.5345%\n",
            "Baseline Epoch [791/1000] | Train Loss: 0.9497 | Train Acc: 48.4221% | Test Loss: 0.9469 | Test Acc: 47.3373%\n",
            "Baseline Epoch [801/1000] | Train Loss: 0.9496 | Train Acc: 48.4714% | Test Loss: 0.9468 | Test Acc: 47.3373%\n",
            "Baseline Epoch [811/1000] | Train Loss: 0.9494 | Train Acc: 48.4221% | Test Loss: 0.9467 | Test Acc: 47.3373%\n",
            "Baseline Epoch [821/1000] | Train Loss: 0.9493 | Train Acc: 48.5207% | Test Loss: 0.9466 | Test Acc: 47.3373%\n",
            "Baseline Epoch [831/1000] | Train Loss: 0.9490 | Train Acc: 48.4714% | Test Loss: 0.9466 | Test Acc: 47.3373%\n",
            "Baseline Epoch [841/1000] | Train Loss: 0.9490 | Train Acc: 48.5207% | Test Loss: 0.9465 | Test Acc: 47.1400%\n",
            "Baseline Epoch [851/1000] | Train Loss: 0.9489 | Train Acc: 48.4221% | Test Loss: 0.9464 | Test Acc: 47.1400%\n",
            "Baseline Epoch [861/1000] | Train Loss: 0.9488 | Train Acc: 48.5207% | Test Loss: 0.9464 | Test Acc: 47.1400%\n",
            "Baseline Epoch [871/1000] | Train Loss: 0.9486 | Train Acc: 48.4714% | Test Loss: 0.9463 | Test Acc: 47.1400%\n",
            "Baseline Epoch [881/1000] | Train Loss: 0.9486 | Train Acc: 48.3728% | Test Loss: 0.9462 | Test Acc: 47.3373%\n",
            "Baseline Epoch [891/1000] | Train Loss: 0.9485 | Train Acc: 48.3728% | Test Loss: 0.9462 | Test Acc: 47.3373%\n",
            "Baseline Epoch [901/1000] | Train Loss: 0.9484 | Train Acc: 48.2742% | Test Loss: 0.9461 | Test Acc: 47.3373%\n",
            "Baseline Epoch [911/1000] | Train Loss: 0.9483 | Train Acc: 48.2742% | Test Loss: 0.9460 | Test Acc: 47.1400%\n",
            "Baseline Epoch [921/1000] | Train Loss: 0.9483 | Train Acc: 48.3235% | Test Loss: 0.9460 | Test Acc: 47.1400%\n",
            "Baseline Epoch [931/1000] | Train Loss: 0.9482 | Train Acc: 48.2249% | Test Loss: 0.9459 | Test Acc: 47.1400%\n",
            "Baseline Epoch [941/1000] | Train Loss: 0.9482 | Train Acc: 48.2249% | Test Loss: 0.9458 | Test Acc: 47.1400%\n",
            "Baseline Epoch [951/1000] | Train Loss: 0.9480 | Train Acc: 48.3235% | Test Loss: 0.9458 | Test Acc: 47.1400%\n",
            "Baseline Epoch [961/1000] | Train Loss: 0.9478 | Train Acc: 48.3235% | Test Loss: 0.9457 | Test Acc: 47.1400%\n",
            "Baseline Epoch [971/1000] | Train Loss: 0.9477 | Train Acc: 48.4221% | Test Loss: 0.9457 | Test Acc: 46.9428%\n",
            "Baseline Epoch [981/1000] | Train Loss: 0.9475 | Train Acc: 48.3235% | Test Loss: 0.9456 | Test Acc: 46.7456%\n",
            "Baseline Epoch [991/1000] | Train Loss: 0.9474 | Train Acc: 48.3235% | Test Loss: 0.9456 | Test Acc: 46.5483%\n",
            "\n",
            "Best baseline model saved to: best_baseline_model.pth with Test Loss: 0.9455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Medium Neural Network Training"
      ],
      "metadata": {
        "id": "gZXS9zB4OKib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MediumClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(MediumClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)  # No activation (CrossEntropyLoss expects raw logits)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "input_dim = len(features)\n",
        "output_dim = 3  # Buy, Sell, Hold\n",
        "mediumClassifier = MediumClassifier(input_dim, output_dim)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
        "optimizer = optim.Adam(mediumClassifier.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "IpvCb8TBOQ7i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "num_epochs = 1000\n",
        "best_val_loss = float(\"inf\")  # Track lowest validation loss\n",
        "best_model_path = \"best_stock_model.pth\"  # Save model path\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    mediumClassifier.train()\n",
        "    total_loss = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    # Training loop\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mediumClassifier(inputs)\n",
        "        loss = criterion(outputs, labels + 1)  # Shift labels (-1,0,1) â†’ (0,1,2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Compute training accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_train += (predicted == (labels + 1)).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    train_accuracy = correct_train / total_train\n",
        "\n",
        "    # Validation loop\n",
        "    mediumClassifier.eval()\n",
        "    total_val_loss = 0\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = mediumClassifier(inputs)\n",
        "            val_loss = criterion(outputs, labels + 1)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            # Compute test accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_test += (predicted == (labels + 1)).sum().item()\n",
        "            total_test += labels.size(0)\n",
        "\n",
        "    val_accuracy = correct_test / total_test\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    avg_val_loss = total_val_loss / len(test_loader)\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(mediumClassifier.state_dict(), best_model_path)\n",
        "\n",
        "    # Print epoch summary\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
        "            f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4%} | \"\n",
        "            f\"Test Loss: {avg_val_loss:.4f} | Test Acc: {val_accuracy:.4%}\")\n",
        "\n",
        "print(f\"\\nBest model saved to: {best_model_path} with Test Loss: {best_val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bIJ7j72OVMc",
        "outputId": "973d7407-f6dd-4b69-a1ef-ebc3a3d6a6cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1000] | Train Loss: 1.1029 | Train Acc: 34.1716% | Test Loss: 1.0914 | Test Acc: 43.1953%\n",
            "Epoch [11/1000] | Train Loss: 0.9791 | Train Acc: 44.6746% | Test Loss: 0.9737 | Test Acc: 45.1677%\n",
            "Epoch [21/1000] | Train Loss: 0.9556 | Train Acc: 46.4990% | Test Loss: 0.9570 | Test Acc: 42.9980%\n",
            "Epoch [31/1000] | Train Loss: 0.9497 | Train Acc: 46.9921% | Test Loss: 0.9529 | Test Acc: 45.1677%\n",
            "Epoch [41/1000] | Train Loss: 0.9470 | Train Acc: 48.0769% | Test Loss: 0.9501 | Test Acc: 45.5621%\n",
            "Epoch [51/1000] | Train Loss: 0.9446 | Train Acc: 48.4714% | Test Loss: 0.9489 | Test Acc: 46.5483%\n",
            "Epoch [61/1000] | Train Loss: 0.9427 | Train Acc: 48.5700% | Test Loss: 0.9469 | Test Acc: 45.9566%\n",
            "Epoch [71/1000] | Train Loss: 0.9405 | Train Acc: 48.6686% | Test Loss: 0.9449 | Test Acc: 46.5483%\n",
            "Epoch [81/1000] | Train Loss: 0.9381 | Train Acc: 49.0631% | Test Loss: 0.9428 | Test Acc: 47.9290%\n",
            "Epoch [91/1000] | Train Loss: 0.9354 | Train Acc: 49.8028% | Test Loss: 0.9399 | Test Acc: 48.7179%\n",
            "Epoch [101/1000] | Train Loss: 0.9317 | Train Acc: 50.5424% | Test Loss: 0.9365 | Test Acc: 49.1124%\n",
            "Epoch [111/1000] | Train Loss: 0.9270 | Train Acc: 51.4300% | Test Loss: 0.9320 | Test Acc: 49.3097%\n",
            "Epoch [121/1000] | Train Loss: 0.9211 | Train Acc: 52.7613% | Test Loss: 0.9263 | Test Acc: 51.6765%\n",
            "Epoch [131/1000] | Train Loss: 0.9137 | Train Acc: 54.7830% | Test Loss: 0.9185 | Test Acc: 52.8600%\n",
            "Epoch [141/1000] | Train Loss: 0.9023 | Train Acc: 56.1637% | Test Loss: 0.9083 | Test Acc: 55.2268%\n",
            "Epoch [151/1000] | Train Loss: 0.8903 | Train Acc: 58.8264% | Test Loss: 0.8954 | Test Acc: 58.9744%\n",
            "Epoch [161/1000] | Train Loss: 0.8769 | Train Acc: 61.0947% | Test Loss: 0.8816 | Test Acc: 61.5385%\n",
            "Epoch [171/1000] | Train Loss: 0.8626 | Train Acc: 63.4122% | Test Loss: 0.8659 | Test Acc: 62.7219%\n",
            "Epoch [181/1000] | Train Loss: 0.8459 | Train Acc: 64.0533% | Test Loss: 0.8501 | Test Acc: 64.4970%\n",
            "Epoch [191/1000] | Train Loss: 0.8302 | Train Acc: 65.5325% | Test Loss: 0.8339 | Test Acc: 66.2722%\n",
            "Epoch [201/1000] | Train Loss: 0.8155 | Train Acc: 65.3846% | Test Loss: 0.8176 | Test Acc: 66.8639%\n",
            "Epoch [211/1000] | Train Loss: 0.8003 | Train Acc: 66.6174% | Test Loss: 0.8040 | Test Acc: 66.4694%\n",
            "Epoch [221/1000] | Train Loss: 0.7862 | Train Acc: 68.3925% | Test Loss: 0.7869 | Test Acc: 69.6252%\n",
            "Epoch [231/1000] | Train Loss: 0.7737 | Train Acc: 69.3294% | Test Loss: 0.7760 | Test Acc: 69.4280%\n",
            "Epoch [241/1000] | Train Loss: 0.7613 | Train Acc: 69.2308% | Test Loss: 0.7628 | Test Acc: 69.6252%\n",
            "Epoch [251/1000] | Train Loss: 0.7501 | Train Acc: 69.2308% | Test Loss: 0.7503 | Test Acc: 71.5976%\n",
            "Epoch [261/1000] | Train Loss: 0.7397 | Train Acc: 71.0552% | Test Loss: 0.7428 | Test Acc: 71.5976%\n",
            "Epoch [271/1000] | Train Loss: 0.7291 | Train Acc: 72.3373% | Test Loss: 0.7291 | Test Acc: 71.5976%\n",
            "Epoch [281/1000] | Train Loss: 0.7212 | Train Acc: 72.4359% | Test Loss: 0.7216 | Test Acc: 71.9921%\n",
            "Epoch [291/1000] | Train Loss: 0.7128 | Train Acc: 71.3511% | Test Loss: 0.7128 | Test Acc: 73.3728%\n",
            "Epoch [301/1000] | Train Loss: 0.7036 | Train Acc: 73.4714% | Test Loss: 0.7051 | Test Acc: 71.7949%\n",
            "Epoch [311/1000] | Train Loss: 0.6967 | Train Acc: 74.1617% | Test Loss: 0.6954 | Test Acc: 74.5562%\n",
            "Epoch [321/1000] | Train Loss: 0.6886 | Train Acc: 75.2465% | Test Loss: 0.6892 | Test Acc: 73.9645%\n",
            "Epoch [331/1000] | Train Loss: 0.6823 | Train Acc: 75.5424% | Test Loss: 0.6829 | Test Acc: 71.5976%\n",
            "Epoch [341/1000] | Train Loss: 0.6776 | Train Acc: 75.8876% | Test Loss: 0.6763 | Test Acc: 71.2032%\n",
            "Epoch [351/1000] | Train Loss: 0.6708 | Train Acc: 74.7041% | Test Loss: 0.6683 | Test Acc: 72.3866%\n",
            "Epoch [361/1000] | Train Loss: 0.6669 | Train Acc: 77.4655% | Test Loss: 0.6624 | Test Acc: 77.5148%\n",
            "Epoch [371/1000] | Train Loss: 0.6599 | Train Acc: 77.3669% | Test Loss: 0.6582 | Test Acc: 77.3176%\n",
            "Epoch [381/1000] | Train Loss: 0.6535 | Train Acc: 75.7890% | Test Loss: 0.6553 | Test Acc: 71.5976%\n",
            "Epoch [391/1000] | Train Loss: 0.6489 | Train Acc: 76.1834% | Test Loss: 0.6461 | Test Acc: 77.5148%\n",
            "Epoch [401/1000] | Train Loss: 0.6443 | Train Acc: 77.4655% | Test Loss: 0.6419 | Test Acc: 76.9231%\n",
            "Epoch [411/1000] | Train Loss: 0.6403 | Train Acc: 78.4024% | Test Loss: 0.6374 | Test Acc: 78.6982%\n",
            "Epoch [421/1000] | Train Loss: 0.6374 | Train Acc: 76.1341% | Test Loss: 0.6308 | Test Acc: 78.6982%\n",
            "Epoch [431/1000] | Train Loss: 0.6319 | Train Acc: 79.3393% | Test Loss: 0.6264 | Test Acc: 77.7120%\n",
            "Epoch [441/1000] | Train Loss: 0.6277 | Train Acc: 76.1834% | Test Loss: 0.6236 | Test Acc: 77.7120%\n",
            "Epoch [451/1000] | Train Loss: 0.6244 | Train Acc: 78.4024% | Test Loss: 0.6220 | Test Acc: 78.3037%\n",
            "Epoch [461/1000] | Train Loss: 0.6201 | Train Acc: 78.1065% | Test Loss: 0.6163 | Test Acc: 78.6982%\n",
            "Epoch [471/1000] | Train Loss: 0.6165 | Train Acc: 79.4379% | Test Loss: 0.6146 | Test Acc: 76.9231%\n",
            "Epoch [481/1000] | Train Loss: 0.6141 | Train Acc: 78.2051% | Test Loss: 0.6133 | Test Acc: 78.6982%\n",
            "Epoch [491/1000] | Train Loss: 0.6102 | Train Acc: 78.2544% | Test Loss: 0.6040 | Test Acc: 78.8955%\n",
            "Epoch [501/1000] | Train Loss: 0.6074 | Train Acc: 74.6548% | Test Loss: 0.6033 | Test Acc: 74.3590%\n",
            "Epoch [511/1000] | Train Loss: 0.6049 | Train Acc: 77.7120% | Test Loss: 0.5994 | Test Acc: 79.2899%\n",
            "Epoch [521/1000] | Train Loss: 0.5998 | Train Acc: 79.2406% | Test Loss: 0.5952 | Test Acc: 78.3037%\n",
            "Epoch [531/1000] | Train Loss: 0.5986 | Train Acc: 77.2682% | Test Loss: 0.5906 | Test Acc: 79.0927%\n",
            "Epoch [541/1000] | Train Loss: 0.5929 | Train Acc: 78.9448% | Test Loss: 0.5902 | Test Acc: 74.3590%\n",
            "Epoch [551/1000] | Train Loss: 0.5919 | Train Acc: 78.6489% | Test Loss: 0.5878 | Test Acc: 76.1341%\n",
            "Epoch [561/1000] | Train Loss: 0.5886 | Train Acc: 78.6982% | Test Loss: 0.5849 | Test Acc: 78.5010%\n",
            "Epoch [571/1000] | Train Loss: 0.5843 | Train Acc: 80.5227% | Test Loss: 0.5856 | Test Acc: 73.1755%\n",
            "Epoch [581/1000] | Train Loss: 0.5834 | Train Acc: 79.1420% | Test Loss: 0.5808 | Test Acc: 80.0789%\n",
            "Epoch [591/1000] | Train Loss: 0.5797 | Train Acc: 79.7337% | Test Loss: 0.5746 | Test Acc: 79.6844%\n",
            "Epoch [601/1000] | Train Loss: 0.5779 | Train Acc: 79.2406% | Test Loss: 0.5733 | Test Acc: 76.5286%\n",
            "Epoch [611/1000] | Train Loss: 0.5742 | Train Acc: 81.2130% | Test Loss: 0.5717 | Test Acc: 79.8817%\n",
            "Epoch [621/1000] | Train Loss: 0.5723 | Train Acc: 80.6213% | Test Loss: 0.5686 | Test Acc: 80.4734%\n",
            "Epoch [631/1000] | Train Loss: 0.5714 | Train Acc: 78.7475% | Test Loss: 0.5656 | Test Acc: 78.5010%\n",
            "Epoch [641/1000] | Train Loss: 0.5694 | Train Acc: 77.7613% | Test Loss: 0.5669 | Test Acc: 78.5010%\n",
            "Epoch [651/1000] | Train Loss: 0.5635 | Train Acc: 82.5444% | Test Loss: 0.5602 | Test Acc: 78.6982%\n",
            "Epoch [661/1000] | Train Loss: 0.5634 | Train Acc: 76.7258% | Test Loss: 0.5569 | Test Acc: 80.0789%\n",
            "Epoch [671/1000] | Train Loss: 0.5594 | Train Acc: 80.8679% | Test Loss: 0.5572 | Test Acc: 78.1065%\n",
            "Epoch [681/1000] | Train Loss: 0.5586 | Train Acc: 79.8323% | Test Loss: 0.5553 | Test Acc: 81.6568%\n",
            "Epoch [691/1000] | Train Loss: 0.5630 | Train Acc: 73.7179% | Test Loss: 0.5580 | Test Acc: 79.8817%\n",
            "Epoch [701/1000] | Train Loss: 0.5534 | Train Acc: 82.0513% | Test Loss: 0.5507 | Test Acc: 81.2623%\n",
            "Epoch [711/1000] | Train Loss: 0.5496 | Train Acc: 81.4103% | Test Loss: 0.5467 | Test Acc: 82.4458%\n",
            "Epoch [721/1000] | Train Loss: 0.5483 | Train Acc: 81.4103% | Test Loss: 0.5477 | Test Acc: 80.8679%\n",
            "Epoch [731/1000] | Train Loss: 0.5459 | Train Acc: 83.2347% | Test Loss: 0.5425 | Test Acc: 82.0513%\n",
            "Epoch [741/1000] | Train Loss: 0.5428 | Train Acc: 82.2485% | Test Loss: 0.5446 | Test Acc: 77.3176%\n",
            "Epoch [751/1000] | Train Loss: 0.5417 | Train Acc: 80.7692% | Test Loss: 0.5434 | Test Acc: 81.4596%\n",
            "Epoch [761/1000] | Train Loss: 0.5398 | Train Acc: 82.8402% | Test Loss: 0.5376 | Test Acc: 81.0651%\n",
            "Epoch [771/1000] | Train Loss: 0.5396 | Train Acc: 78.6982% | Test Loss: 0.5422 | Test Acc: 78.8955%\n",
            "Epoch [781/1000] | Train Loss: 0.5411 | Train Acc: 78.6489% | Test Loss: 0.5345 | Test Acc: 82.0513%\n",
            "Epoch [791/1000] | Train Loss: 0.5341 | Train Acc: 81.7061% | Test Loss: 0.5329 | Test Acc: 80.6706%\n",
            "Epoch [801/1000] | Train Loss: 0.5309 | Train Acc: 83.3826% | Test Loss: 0.5354 | Test Acc: 77.7120%\n",
            "Epoch [811/1000] | Train Loss: 0.5291 | Train Acc: 83.0375% | Test Loss: 0.5302 | Test Acc: 82.4458%\n",
            "Epoch [821/1000] | Train Loss: 0.5268 | Train Acc: 83.7278% | Test Loss: 0.5312 | Test Acc: 80.4734%\n",
            "Epoch [831/1000] | Train Loss: 0.5256 | Train Acc: 84.0237% | Test Loss: 0.5266 | Test Acc: 82.2485%\n",
            "Epoch [841/1000] | Train Loss: 0.5242 | Train Acc: 82.8402% | Test Loss: 0.5347 | Test Acc: 77.7120%\n",
            "Epoch [851/1000] | Train Loss: 0.5210 | Train Acc: 83.4320% | Test Loss: 0.5204 | Test Acc: 81.0651%\n",
            "Epoch [861/1000] | Train Loss: 0.5191 | Train Acc: 83.7278% | Test Loss: 0.5203 | Test Acc: 82.6430%\n",
            "Epoch [871/1000] | Train Loss: 0.5183 | Train Acc: 83.0375% | Test Loss: 0.5195 | Test Acc: 82.8402%\n",
            "Epoch [881/1000] | Train Loss: 0.5169 | Train Acc: 82.4458% | Test Loss: 0.5203 | Test Acc: 83.4320%\n",
            "Epoch [891/1000] | Train Loss: 0.5145 | Train Acc: 82.7416% | Test Loss: 0.5169 | Test Acc: 83.2347%\n",
            "Epoch [901/1000] | Train Loss: 0.5117 | Train Acc: 84.8126% | Test Loss: 0.5163 | Test Acc: 82.2485%\n",
            "Epoch [911/1000] | Train Loss: 0.5125 | Train Acc: 82.2485% | Test Loss: 0.5142 | Test Acc: 82.4458%\n",
            "Epoch [921/1000] | Train Loss: 0.5101 | Train Acc: 83.6785% | Test Loss: 0.5116 | Test Acc: 83.6292%\n",
            "Epoch [931/1000] | Train Loss: 0.5095 | Train Acc: 80.6706% | Test Loss: 0.5097 | Test Acc: 83.2347%\n",
            "Epoch [941/1000] | Train Loss: 0.5054 | Train Acc: 84.3688% | Test Loss: 0.5093 | Test Acc: 83.4320%\n",
            "Epoch [951/1000] | Train Loss: 0.5030 | Train Acc: 85.7988% | Test Loss: 0.5069 | Test Acc: 83.4320%\n",
            "Epoch [961/1000] | Train Loss: 0.5021 | Train Acc: 84.5168% | Test Loss: 0.5042 | Test Acc: 83.2347%\n",
            "Epoch [971/1000] | Train Loss: 0.5029 | Train Acc: 82.4951% | Test Loss: 0.5059 | Test Acc: 83.6292%\n",
            "Epoch [981/1000] | Train Loss: 0.5051 | Train Acc: 80.2761% | Test Loss: 0.5055 | Test Acc: 80.6706%\n",
            "Epoch [991/1000] | Train Loss: 0.5003 | Train Acc: 82.2485% | Test Loss: 0.5007 | Test Acc: 83.2347%\n",
            "\n",
            "Best model saved to: best_stock_model.pth with Test Loss: 0.4977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Neural Network Training"
      ],
      "metadata": {
        "id": "TLrQhqV2MuBZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MJZL6mZnueV6"
      },
      "outputs": [],
      "source": [
        "class ImprovedStockClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ImprovedStockClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.bn3 = nn.BatchNorm1d(32)\n",
        "        self.dropout3 = nn.Dropout(0.2)\n",
        "\n",
        "        self.fc4 = nn.Linear(32, output_dim)\n",
        "\n",
        "        self.activation = nn.LeakyReLU(negative_slope=0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.activation(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.activation(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "improved_model = ImprovedStockClassifier(input_dim, output_dim)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(improved_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1000\n",
        "best_improved_loss = float(\"inf\")\n",
        "improved_model_path = \"best_improved_model.pth\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    improved_model.train()\n",
        "    total_loss = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = improved_model(inputs)\n",
        "        loss = criterion(outputs, labels + 1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_train += (predicted == (labels + 1)).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    train_accuracy = correct_train / total_train\n",
        "\n",
        "    improved_model.eval()\n",
        "    total_val_loss = 0\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = improved_model(inputs)\n",
        "            val_loss = criterion(outputs, labels + 1)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_test += (predicted == (labels + 1)).sum().item()\n",
        "            total_test += labels.size(0)\n",
        "\n",
        "    val_accuracy = correct_test / total_test\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    avg_val_loss = total_val_loss / len(test_loader)\n",
        "\n",
        "    if avg_val_loss < best_improved_loss:\n",
        "        best_improved_loss = avg_val_loss\n",
        "        torch.save(improved_model.state_dict(), improved_model_path)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Improved Model - Epoch [{epoch+1}/{num_epochs}] | \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4%} | \"\n",
        "              f\"Test Loss: {avg_val_loss:.4f} | Test Acc: {val_accuracy:.4%}\")\n",
        "\n",
        "print(f\"\\nBest improved model saved to: {improved_model_path} with Test Loss: {best_improved_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqqsbashHNWL",
        "outputId": "5950301c-987e-41cb-b3ca-16db1e043687"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improved Model - Epoch [1/1000] | Train Loss: 1.2709 | Train Acc: 18.6391% | Test Loss: 1.1662 | Test Acc: 10.0592%\n",
            "Improved Model - Epoch [11/1000] | Train Loss: 1.0566 | Train Acc: 41.7160% | Test Loss: 1.0807 | Test Acc: 39.4477%\n",
            "Improved Model - Epoch [21/1000] | Train Loss: 0.9846 | Train Acc: 49.0631% | Test Loss: 0.9780 | Test Acc: 48.9152%\n",
            "Improved Model - Epoch [31/1000] | Train Loss: 0.9236 | Train Acc: 53.7475% | Test Loss: 0.9197 | Test Acc: 55.6213%\n",
            "Improved Model - Epoch [41/1000] | Train Loss: 0.8581 | Train Acc: 56.5089% | Test Loss: 0.8531 | Test Acc: 60.1578%\n",
            "Improved Model - Epoch [51/1000] | Train Loss: 0.8086 | Train Acc: 59.3688% | Test Loss: 0.7570 | Test Acc: 63.5108%\n",
            "Improved Model - Epoch [61/1000] | Train Loss: 0.7805 | Train Acc: 60.1578% | Test Loss: 0.7163 | Test Acc: 64.4970%\n",
            "Improved Model - Epoch [71/1000] | Train Loss: 0.7581 | Train Acc: 61.4398% | Test Loss: 0.6867 | Test Acc: 67.8501%\n",
            "Improved Model - Epoch [81/1000] | Train Loss: 0.7397 | Train Acc: 63.8560% | Test Loss: 0.6791 | Test Acc: 65.8777%\n",
            "Improved Model - Epoch [91/1000] | Train Loss: 0.7190 | Train Acc: 65.6805% | Test Loss: 0.6611 | Test Acc: 66.6667%\n",
            "Improved Model - Epoch [101/1000] | Train Loss: 0.7086 | Train Acc: 65.6805% | Test Loss: 0.6364 | Test Acc: 68.0473%\n",
            "Improved Model - Epoch [111/1000] | Train Loss: 0.6958 | Train Acc: 67.2584% | Test Loss: 0.6220 | Test Acc: 71.4004%\n",
            "Improved Model - Epoch [121/1000] | Train Loss: 0.6871 | Train Acc: 67.2584% | Test Loss: 0.6149 | Test Acc: 72.5838%\n",
            "Improved Model - Epoch [131/1000] | Train Loss: 0.6892 | Train Acc: 66.8146% | Test Loss: 0.6014 | Test Acc: 71.9921%\n",
            "Improved Model - Epoch [141/1000] | Train Loss: 0.6580 | Train Acc: 70.7101% | Test Loss: 0.5942 | Test Acc: 73.5700%\n",
            "Improved Model - Epoch [151/1000] | Train Loss: 0.6706 | Train Acc: 70.1183% | Test Loss: 0.5783 | Test Acc: 78.5010%\n",
            "Improved Model - Epoch [161/1000] | Train Loss: 0.6531 | Train Acc: 71.0059% | Test Loss: 0.5697 | Test Acc: 78.6982%\n",
            "Improved Model - Epoch [171/1000] | Train Loss: 0.6543 | Train Acc: 71.6963% | Test Loss: 0.5656 | Test Acc: 77.9093%\n",
            "Improved Model - Epoch [181/1000] | Train Loss: 0.6458 | Train Acc: 73.0769% | Test Loss: 0.5614 | Test Acc: 79.4872%\n",
            "Improved Model - Epoch [191/1000] | Train Loss: 0.6478 | Train Acc: 71.8935% | Test Loss: 0.5509 | Test Acc: 81.8540%\n",
            "Improved Model - Epoch [201/1000] | Train Loss: 0.6426 | Train Acc: 72.8304% | Test Loss: 0.5443 | Test Acc: 82.0513%\n",
            "Improved Model - Epoch [211/1000] | Train Loss: 0.6357 | Train Acc: 73.1262% | Test Loss: 0.5405 | Test Acc: 83.6292%\n",
            "Improved Model - Epoch [221/1000] | Train Loss: 0.6403 | Train Acc: 72.8797% | Test Loss: 0.5371 | Test Acc: 86.7850%\n",
            "Improved Model - Epoch [231/1000] | Train Loss: 0.6324 | Train Acc: 73.0276% | Test Loss: 0.5358 | Test Acc: 82.4458%\n",
            "Improved Model - Epoch [241/1000] | Train Loss: 0.6289 | Train Acc: 72.3866% | Test Loss: 0.5344 | Test Acc: 83.4320%\n",
            "Improved Model - Epoch [251/1000] | Train Loss: 0.6304 | Train Acc: 73.3235% | Test Loss: 0.5279 | Test Acc: 85.7988%\n",
            "Improved Model - Epoch [261/1000] | Train Loss: 0.6265 | Train Acc: 72.9783% | Test Loss: 0.5263 | Test Acc: 87.3767%\n",
            "Improved Model - Epoch [271/1000] | Train Loss: 0.6327 | Train Acc: 73.1262% | Test Loss: 0.5257 | Test Acc: 87.3767%\n",
            "Improved Model - Epoch [281/1000] | Train Loss: 0.6088 | Train Acc: 73.6686% | Test Loss: 0.5232 | Test Acc: 88.1657%\n",
            "Improved Model - Epoch [291/1000] | Train Loss: 0.6239 | Train Acc: 72.5345% | Test Loss: 0.5181 | Test Acc: 87.5740%\n",
            "Improved Model - Epoch [301/1000] | Train Loss: 0.6056 | Train Acc: 75.7890% | Test Loss: 0.5085 | Test Acc: 87.7712%\n",
            "Improved Model - Epoch [311/1000] | Train Loss: 0.6146 | Train Acc: 74.1124% | Test Loss: 0.5086 | Test Acc: 88.3629%\n",
            "Improved Model - Epoch [321/1000] | Train Loss: 0.6110 | Train Acc: 74.3590% | Test Loss: 0.5082 | Test Acc: 88.3629%\n",
            "Improved Model - Epoch [331/1000] | Train Loss: 0.6192 | Train Acc: 74.1617% | Test Loss: 0.5086 | Test Acc: 88.1657%\n",
            "Improved Model - Epoch [341/1000] | Train Loss: 0.6169 | Train Acc: 74.9507% | Test Loss: 0.5056 | Test Acc: 88.1657%\n",
            "Improved Model - Epoch [351/1000] | Train Loss: 0.6140 | Train Acc: 74.7535% | Test Loss: 0.5052 | Test Acc: 88.3629%\n",
            "Improved Model - Epoch [361/1000] | Train Loss: 0.6018 | Train Acc: 75.5424% | Test Loss: 0.5052 | Test Acc: 88.3629%\n",
            "Improved Model - Epoch [371/1000] | Train Loss: 0.6175 | Train Acc: 74.1617% | Test Loss: 0.5058 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [381/1000] | Train Loss: 0.5908 | Train Acc: 76.2821% | Test Loss: 0.5042 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [391/1000] | Train Loss: 0.6061 | Train Acc: 75.1972% | Test Loss: 0.5027 | Test Acc: 88.3629%\n",
            "Improved Model - Epoch [401/1000] | Train Loss: 0.6103 | Train Acc: 75.1972% | Test Loss: 0.5003 | Test Acc: 88.7574%\n",
            "Improved Model - Epoch [411/1000] | Train Loss: 0.6186 | Train Acc: 75.2465% | Test Loss: 0.5025 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [421/1000] | Train Loss: 0.6043 | Train Acc: 74.8521% | Test Loss: 0.5018 | Test Acc: 88.3629%\n",
            "Improved Model - Epoch [431/1000] | Train Loss: 0.6019 | Train Acc: 74.5069% | Test Loss: 0.5009 | Test Acc: 88.3629%\n",
            "Improved Model - Epoch [441/1000] | Train Loss: 0.6148 | Train Acc: 74.3590% | Test Loss: 0.5002 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [451/1000] | Train Loss: 0.5972 | Train Acc: 75.4438% | Test Loss: 0.5013 | Test Acc: 88.7574%\n",
            "Improved Model - Epoch [461/1000] | Train Loss: 0.6134 | Train Acc: 74.7535% | Test Loss: 0.4996 | Test Acc: 88.3629%\n",
            "Improved Model - Epoch [471/1000] | Train Loss: 0.6022 | Train Acc: 75.0000% | Test Loss: 0.4999 | Test Acc: 88.1657%\n",
            "Improved Model - Epoch [481/1000] | Train Loss: 0.6092 | Train Acc: 75.2959% | Test Loss: 0.5004 | Test Acc: 88.7574%\n",
            "Improved Model - Epoch [491/1000] | Train Loss: 0.6141 | Train Acc: 74.8028% | Test Loss: 0.5012 | Test Acc: 88.3629%\n",
            "Improved Model - Epoch [501/1000] | Train Loss: 0.6062 | Train Acc: 75.9862% | Test Loss: 0.4986 | Test Acc: 88.7574%\n",
            "Improved Model - Epoch [511/1000] | Train Loss: 0.6077 | Train Acc: 75.2959% | Test Loss: 0.4998 | Test Acc: 88.7574%\n",
            "Improved Model - Epoch [521/1000] | Train Loss: 0.6144 | Train Acc: 74.0631% | Test Loss: 0.4993 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [531/1000] | Train Loss: 0.6103 | Train Acc: 75.0493% | Test Loss: 0.4999 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [541/1000] | Train Loss: 0.6221 | Train Acc: 74.3097% | Test Loss: 0.4994 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [551/1000] | Train Loss: 0.6008 | Train Acc: 74.9507% | Test Loss: 0.4993 | Test Acc: 88.7574%\n",
            "Improved Model - Epoch [561/1000] | Train Loss: 0.6166 | Train Acc: 74.4083% | Test Loss: 0.5004 | Test Acc: 88.7574%\n",
            "Improved Model - Epoch [571/1000] | Train Loss: 0.6164 | Train Acc: 74.6548% | Test Loss: 0.4988 | Test Acc: 88.7574%\n",
            "Improved Model - Epoch [581/1000] | Train Loss: 0.6097 | Train Acc: 73.9152% | Test Loss: 0.4991 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [591/1000] | Train Loss: 0.5984 | Train Acc: 76.0848% | Test Loss: 0.5007 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [601/1000] | Train Loss: 0.6105 | Train Acc: 75.0000% | Test Loss: 0.4983 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [611/1000] | Train Loss: 0.6048 | Train Acc: 75.6410% | Test Loss: 0.4994 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [621/1000] | Train Loss: 0.5978 | Train Acc: 74.8521% | Test Loss: 0.4997 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [631/1000] | Train Loss: 0.6028 | Train Acc: 76.0355% | Test Loss: 0.4999 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [641/1000] | Train Loss: 0.6057 | Train Acc: 75.4931% | Test Loss: 0.4987 | Test Acc: 88.3629%\n",
            "Improved Model - Epoch [651/1000] | Train Loss: 0.6062 | Train Acc: 75.1479% | Test Loss: 0.4998 | Test Acc: 88.7574%\n",
            "Improved Model - Epoch [661/1000] | Train Loss: 0.5982 | Train Acc: 75.5917% | Test Loss: 0.4979 | Test Acc: 88.1657%\n",
            "Improved Model - Epoch [671/1000] | Train Loss: 0.6174 | Train Acc: 72.7318% | Test Loss: 0.4987 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [681/1000] | Train Loss: 0.6122 | Train Acc: 73.4714% | Test Loss: 0.4992 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [691/1000] | Train Loss: 0.6126 | Train Acc: 74.6055% | Test Loss: 0.4998 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [701/1000] | Train Loss: 0.5997 | Train Acc: 76.5779% | Test Loss: 0.5006 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [711/1000] | Train Loss: 0.6092 | Train Acc: 74.9507% | Test Loss: 0.4994 | Test Acc: 88.7574%\n",
            "Improved Model - Epoch [721/1000] | Train Loss: 0.6006 | Train Acc: 75.1479% | Test Loss: 0.4987 | Test Acc: 88.7574%\n",
            "Improved Model - Epoch [731/1000] | Train Loss: 0.6054 | Train Acc: 75.0000% | Test Loss: 0.4977 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [741/1000] | Train Loss: 0.6226 | Train Acc: 74.0631% | Test Loss: 0.4997 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [751/1000] | Train Loss: 0.6243 | Train Acc: 73.8166% | Test Loss: 0.5011 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [761/1000] | Train Loss: 0.6096 | Train Acc: 75.0000% | Test Loss: 0.5010 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [771/1000] | Train Loss: 0.6139 | Train Acc: 75.0986% | Test Loss: 0.5015 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [781/1000] | Train Loss: 0.6087 | Train Acc: 75.3452% | Test Loss: 0.4991 | Test Acc: 88.7574%\n",
            "Improved Model - Epoch [791/1000] | Train Loss: 0.6009 | Train Acc: 75.1479% | Test Loss: 0.4978 | Test Acc: 88.7574%\n",
            "Improved Model - Epoch [801/1000] | Train Loss: 0.5986 | Train Acc: 75.4438% | Test Loss: 0.4985 | Test Acc: 88.3629%\n",
            "Improved Model - Epoch [811/1000] | Train Loss: 0.6279 | Train Acc: 74.4576% | Test Loss: 0.5004 | Test Acc: 88.7574%\n",
            "Improved Model - Epoch [821/1000] | Train Loss: 0.6104 | Train Acc: 75.9369% | Test Loss: 0.5003 | Test Acc: 88.1657%\n",
            "Improved Model - Epoch [831/1000] | Train Loss: 0.6124 | Train Acc: 73.3235% | Test Loss: 0.4990 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [841/1000] | Train Loss: 0.6024 | Train Acc: 74.8521% | Test Loss: 0.5009 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [851/1000] | Train Loss: 0.6091 | Train Acc: 75.2465% | Test Loss: 0.4978 | Test Acc: 88.3629%\n",
            "Improved Model - Epoch [861/1000] | Train Loss: 0.5900 | Train Acc: 75.7396% | Test Loss: 0.4990 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [871/1000] | Train Loss: 0.6038 | Train Acc: 76.0848% | Test Loss: 0.4997 | Test Acc: 88.7574%\n",
            "Improved Model - Epoch [881/1000] | Train Loss: 0.6096 | Train Acc: 74.9507% | Test Loss: 0.4994 | Test Acc: 88.3629%\n",
            "Improved Model - Epoch [891/1000] | Train Loss: 0.6030 | Train Acc: 75.7396% | Test Loss: 0.4990 | Test Acc: 88.7574%\n",
            "Improved Model - Epoch [901/1000] | Train Loss: 0.6003 | Train Acc: 75.4931% | Test Loss: 0.5010 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [911/1000] | Train Loss: 0.6019 | Train Acc: 76.0355% | Test Loss: 0.5006 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [921/1000] | Train Loss: 0.6025 | Train Acc: 75.0986% | Test Loss: 0.4989 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [931/1000] | Train Loss: 0.5956 | Train Acc: 75.3452% | Test Loss: 0.4982 | Test Acc: 88.7574%\n",
            "Improved Model - Epoch [941/1000] | Train Loss: 0.6158 | Train Acc: 75.3452% | Test Loss: 0.4991 | Test Acc: 88.3629%\n",
            "Improved Model - Epoch [951/1000] | Train Loss: 0.6060 | Train Acc: 75.0986% | Test Loss: 0.4984 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [961/1000] | Train Loss: 0.6074 | Train Acc: 74.7535% | Test Loss: 0.4983 | Test Acc: 88.5602%\n",
            "Improved Model - Epoch [971/1000] | Train Loss: 0.6095 | Train Acc: 74.1124% | Test Loss: 0.4991 | Test Acc: 88.3629%\n",
            "Improved Model - Epoch [981/1000] | Train Loss: 0.6116 | Train Acc: 74.4083% | Test Loss: 0.4985 | Test Acc: 88.1657%\n",
            "Improved Model - Epoch [991/1000] | Train Loss: 0.6097 | Train Acc: 75.5424% | Test Loss: 0.4997 | Test Acc: 88.5602%\n",
            "\n",
            "Best improved model saved to: best_improved_model.pth with Test Loss: 0.4972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare and Evalutate both models"
      ],
      "metadata": {
        "id": "Ir0KMvfLNL6F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyZPhQn0ulan",
        "outputId": "6b03397f-2f68-47b0-ade7-f6dd182b7c23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline NN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Sell       0.45      0.58      0.51       229\n",
            "        Hold       0.00      0.00      0.00        51\n",
            "         Buy       0.49      0.46      0.48       227\n",
            "\n",
            "    accuracy                           0.47       507\n",
            "   macro avg       0.31      0.35      0.33       507\n",
            "weighted avg       0.42      0.47      0.44       507\n",
            "\n",
            "Baseline NN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Sell       0.85      0.91      0.88       229\n",
            "        Hold       0.71      0.10      0.17        51\n",
            "         Buy       0.83      0.92      0.87       227\n",
            "\n",
            "    accuracy                           0.83       507\n",
            "   macro avg       0.80      0.64      0.64       507\n",
            "weighted avg       0.82      0.83      0.80       507\n",
            "\n",
            "Advanced NN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Sell       0.90      0.98      0.94       229\n",
            "        Hold       1.00      0.04      0.08        51\n",
            "         Buy       0.87      0.98      0.92       227\n",
            "\n",
            "    accuracy                           0.89       507\n",
            "   macro avg       0.92      0.67      0.65       507\n",
            "weighted avg       0.90      0.89      0.84       507\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, test_loader, name=\"Model\"):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predictions.extend(predicted.numpy())\n",
        "            actuals.extend((labels + 1).numpy())  # Shift back from (-1,0,1) to (0,1,2)\n",
        "\n",
        "    print(f\"{name} Classification Report:\")\n",
        "    print(classification_report(actuals, predictions, target_names=['Sell', 'Hold', 'Buy']))\n",
        "    return predictions, actuals\n",
        "\n",
        "# Evaluate both models\n",
        "predictions_baseline, actuals_baseline = evaluate_model(baseline_model, test_loader, name=\"Baseline NN\")\n",
        "predictions_baseline, actuals_baseline = evaluate_model(mediumClassifier, test_loader, name=\"Baseline NN\")\n",
        "predictions_advanced, actuals_advanced = evaluate_model(improved_model, test_loader, name=\"Advanced NN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY7Pjlb5uxbz",
        "outputId": "bffef682-a24f-4e75-a3ff-3cee7ea45ff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline model: Today's recommended action for TSLA: Buy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "latest_data = stock_data.iloc[-1][['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "latest_sp500 = sp500_data.iloc[-1]['Close']\n",
        "\n",
        "latest_features = np.array([latest_data['Open'], latest_data['High'], latest_data['Low'], latest_data['Close'], latest_data['Volume'], latest_sp500])\n",
        "latest_features = latest_features.reshape(1, -1)\n",
        "latest_features_scaled = scaler.transform(latest_features)\n",
        "latest_tensor = torch.tensor(latest_features_scaled, dtype=torch.float32)\n",
        "\n",
        "baseline_model.eval()\n",
        "with torch.no_grad():\n",
        "    output = baseline_model(latest_tensor)\n",
        "    _, predicted_class = torch.max(output, 1)\n",
        "\n",
        "label_map = {0: \"Sell\", 1: \"Hold\", 2: \"Buy\"}\n",
        "predicted_label = label_map[predicted_class.item()]\n",
        "\n",
        "print(f\"Baseline model: Today's recommended action for {stock_ticker}: {predicted_label}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latest_data = stock_data.iloc[-1][['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "latest_sp500 = sp500_data.iloc[-1]['Close']\n",
        "\n",
        "latest_features = np.array([latest_data['Open'], latest_data['High'], latest_data['Low'], latest_data['Close'], latest_data['Volume'], latest_sp500])\n",
        "latest_features = latest_features.reshape(1, -1)\n",
        "latest_features_scaled = scaler.transform(latest_features)\n",
        "latest_tensor = torch.tensor(latest_features_scaled, dtype=torch.float32)\n",
        "\n",
        "mediumClassifier.eval()\n",
        "with torch.no_grad():\n",
        "    output = mediumClassifier(latest_tensor)\n",
        "    _, predicted_class = torch.max(output, 1)\n",
        "\n",
        "label_map = {0: \"Sell\", 1: \"Hold\", 2: \"Buy\"}\n",
        "predicted_label = label_map[predicted_class.item()]\n",
        "\n",
        "print(f\"Medium model: Today's recommended action for {stock_ticker}: {predicted_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyS8-_-ROuJc",
        "outputId": "9423a761-5ff2-4f1d-fe30-539045955501"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medium model: Today's recommended action for TSLA: Buy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a4065ad-cc95-468a-df6d-25ab7cb2eb1f",
        "id": "G1AKWc3gKo5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improved model: Today's recommended action for TSLA: Buy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "latest_data = stock_data.iloc[-1][['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "latest_sp500 = sp500_data.iloc[-1]['Close']\n",
        "\n",
        "latest_features = np.array([latest_data['Open'], latest_data['High'], latest_data['Low'], latest_data['Close'], latest_data['Volume'], latest_sp500])\n",
        "latest_features = latest_features.reshape(1, -1)\n",
        "latest_features_scaled = scaler.transform(latest_features)\n",
        "latest_tensor = torch.tensor(latest_features_scaled, dtype=torch.float32)\n",
        "\n",
        "improved_model.eval()\n",
        "with torch.no_grad():\n",
        "    output = improved_model(latest_tensor)\n",
        "    _, predicted_class = torch.max(output, 1)\n",
        "\n",
        "label_map = {0: \"Sell\", 1: \"Hold\", 2: \"Buy\"}\n",
        "predicted_label = label_map[predicted_class.item()]\n",
        "\n",
        "print(f\"Improved model: Today's recommended action for {stock_ticker}: {predicted_label}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}